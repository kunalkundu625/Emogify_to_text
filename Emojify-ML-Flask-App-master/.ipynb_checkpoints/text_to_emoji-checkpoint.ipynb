{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de43f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,SpatialDropout1D,Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7156bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text():\n",
    "\tfile = 'raw.pickle'\n",
    "\tresponse = requests.get(\"https://raw.githubusercontent.com/bfelbo/DeepMoji/master/data/PsychExp/raw.pickle\")\n",
    "\topen(file, 'wb').write(response.content)\n",
    "\tdata = pickle.load(open(file,'rb'),encoding='latin1')\n",
    "\tif os.path.exists('data.txt'):\n",
    "\t\tos.remove('data.txt')\n",
    "\ttry:\n",
    "\t\ttexts = [str(x) for x in data['texts']]\n",
    "\t\tlabels = [x['label'] for x in data['info']]\n",
    "\t\twith open(\"data.txt\", 'a') as txtfile: \n",
    "\t\t\tfor i in range(len(texts)):\n",
    "\t\t\t\ttxtfile.write(np.array2string(labels[i]))\n",
    "\t\t\t\ttxtfile.write(str(texts[i])+'\\n')\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\ttexts = [x for x in data['texts']]\n",
    "\t\tlabels = [x['label'] for x in data['info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df88fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_name):\n",
    "\tdata_list  = []\n",
    "\twith open(file_name,'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tlabel = ' '.join(line[:line.find(\"]\")].strip().split())\n",
    "\t\t\ttext = line[line.find(\"]\")+1:].strip()\n",
    "\t\t\tdata_list.append([label, text])\n",
    "\n",
    "\treturn data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cb79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(text_list):\n",
    "\tlabel_list = []\n",
    "\ttext_list = [text_list[i][0].replace('[','') for i in range(len(text_list))]\n",
    "\tlabel_list = [list(np.fromstring(text_list[i], dtype=float, sep=' ')) for i in range(len(text_list))]\n",
    "\treturn label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c578465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_msgs(text_list):\n",
    "\tmsg_list = []\n",
    "\tmsg_list = [text_list[i][1] for i in range(len(text_list))]\n",
    "\treturn msg_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca40661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_file):\n",
    "\twith open(glove_file,'r',encoding='UTF-8') as file:\n",
    "\t\twords = set()\n",
    "\t\tword_to_vec = {}\n",
    "\t\tfor line in file:\n",
    "\t\t\tline = line.strip().split()\n",
    "\t\t\tline[0] = re.sub('[^a-zA-Z]', '', line[0])\n",
    "\t\t\tif len(line[0]) > 0:\n",
    "\t\t\t\twords.add(line[0])\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tword_to_vec[line[0]] = np.array(line[1:],dtype=np.float64)\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint('Error has occured')\n",
    "\t\t\t\t\tprint('-'*50)\n",
    "\t\t\t\t\tprint(line[1:])\n",
    "\n",
    "\t\ti = 1\n",
    "\t\tword_to_index = {}\n",
    "\t\tindex_to_word = {}\n",
    "\t\tfor word in sorted(words):\n",
    "\t\t\tword_to_index[word] = i\n",
    "\t\t\tindex_to_word[i] = word\n",
    "\t\t\ti = i+1\n",
    "\treturn word_to_index,index_to_word,word_to_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbdf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(text_arr,word_to_index,max_len):\n",
    "    arr_len = text_arr.shape[0]\n",
    "    arr_indices = np.zeros((arr_len,max_len))\n",
    "    for i in range(arr_len):\n",
    "        sentence = text_arr[i].lower().split()\n",
    "        j = 0\n",
    "        for word in sentence:\n",
    "            if word in word_to_index:\n",
    "                arr_indices[i,j] = word_to_index[word]\n",
    "                j = j+1\n",
    "\n",
    "    return arr_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5114de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index,word_to_vec):\n",
    "\tcorpus_len = len(word_to_index) + 1\n",
    "\tembed_dim = word_to_vec['word'].shape[0]\n",
    "\n",
    "\tembed_matrix = np.zeros((corpus_len,embed_dim))\n",
    "\n",
    "\tfor word, index in word_to_index.items():\n",
    "\t\tembed_matrix[index,:] = word_to_vec[word]\n",
    "\n",
    "\tembedding_layer = Embedding(corpus_len, embed_dim)\n",
    "\tembedding_layer.build((None,))\n",
    "\tembedding_layer.set_weights([embed_matrix])\n",
    "\n",
    "\treturn embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b97052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape,embedding_layer):\n",
    "\tsentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "\tembedding_layer =  embedding_layer\n",
    "\tembeddings = embedding_layer(sentence_indices)\n",
    "\treg = L1L2(0.01, 0.01)\n",
    "\n",
    "\tX = Bidirectional(LSTM(128, return_sequences=True,bias_regularizer=reg,kernel_initializer='he_uniform'))(embeddings)\n",
    "\tX = BatchNormalization()(X)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = LSTM(64)(X)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = Dense(7, activation='softmax')(X)\n",
    "\tX =  Activation('softmax')(X)\n",
    "\tmodel = Model(sentence_indices, X)\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6941ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 50)            17090100  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 256)           183296    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50, 256)           1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 17,357,051\n",
      "Trainable params: 17,356,539\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "187/187 [==============================] - 84s 373ms/step - loss: 6.3897 - accuracy: 0.1664\n",
      "Epoch 2/30\n",
      "187/187 [==============================] - 71s 379ms/step - loss: 5.1505 - accuracy: 0.2510\n",
      "Epoch 3/30\n",
      "187/187 [==============================] - 76s 405ms/step - loss: 4.1263 - accuracy: 0.3112\n",
      "Epoch 4/30\n",
      "187/187 [==============================] - 74s 395ms/step - loss: 3.2951 - accuracy: 0.3728\n",
      "Epoch 5/30\n",
      "187/187 [==============================] - 78s 419ms/step - loss: 2.6330 - accuracy: 0.4328\n",
      "Epoch 6/30\n",
      "187/187 [==============================] - 80s 425ms/step - loss: 2.1102 - accuracy: 0.4935\n",
      "Epoch 7/30\n",
      "187/187 [==============================] - 80s 426ms/step - loss: 1.7185 - accuracy: 0.5456\n",
      "Epoch 8/30\n",
      "187/187 [==============================] - 81s 432ms/step - loss: 1.5952 - accuracy: 0.5765\n",
      "Epoch 9/30\n",
      "187/187 [==============================] - 75s 400ms/step - loss: 1.5498 - accuracy: 0.6252\n",
      "Epoch 10/30\n",
      "187/187 [==============================] - 71s 382ms/step - loss: 1.5152 - accuracy: 0.6573\n",
      "Epoch 11/30\n",
      "187/187 [==============================] - 72s 386ms/step - loss: 1.4961 - accuracy: 0.6743\n",
      "Epoch 12/30\n",
      "187/187 [==============================] - 74s 398ms/step - loss: 1.4752 - accuracy: 0.6984\n",
      "Epoch 13/30\n",
      "187/187 [==============================] - 74s 394ms/step - loss: 1.4517 - accuracy: 0.7172\n",
      "Epoch 14/30\n",
      "187/187 [==============================] - 71s 379ms/step - loss: 1.4289 - accuracy: 0.7420\n",
      "Epoch 15/30\n",
      "187/187 [==============================] - 73s 392ms/step - loss: 1.4094 - accuracy: 0.7629\n",
      "Epoch 16/30\n",
      "187/187 [==============================] - 70s 374ms/step - loss: 1.3989 - accuracy: 0.7736\n",
      "Epoch 17/30\n",
      "187/187 [==============================] - 71s 379ms/step - loss: 1.3899 - accuracy: 0.7802\n",
      "Epoch 18/30\n",
      "187/187 [==============================] - 77s 412ms/step - loss: 1.3890 - accuracy: 0.7814\n",
      "Epoch 19/30\n",
      "187/187 [==============================] - 75s 399ms/step - loss: 1.3721 - accuracy: 0.7980\n",
      "Epoch 20/30\n",
      "187/187 [==============================] - 81s 431ms/step - loss: 1.3719 - accuracy: 0.7965\n",
      "Epoch 21/30\n",
      "187/187 [==============================] - 75s 399ms/step - loss: 1.3493 - accuracy: 0.8200\n",
      "Epoch 22/30\n",
      "187/187 [==============================] - 77s 410ms/step - loss: 1.3382 - accuracy: 0.8316\n",
      "Epoch 23/30\n",
      "187/187 [==============================] - 76s 407ms/step - loss: 1.3359 - accuracy: 0.8344\n",
      "Epoch 24/30\n",
      "187/187 [==============================] - 80s 428ms/step - loss: 1.3294 - accuracy: 0.8392\n",
      "Epoch 25/30\n",
      "187/187 [==============================] - 73s 391ms/step - loss: 1.3222 - accuracy: 0.8483\n",
      "Epoch 26/30\n",
      "187/187 [==============================] - 81s 435ms/step - loss: 1.3186 - accuracy: 0.8508\n",
      "Epoch 27/30\n",
      "187/187 [==============================] - 77s 413ms/step - loss: 1.3082 - accuracy: 0.8613\n",
      "Epoch 28/30\n",
      "187/187 [==============================] - 76s 407ms/step - loss: 1.3117 - accuracy: 0.8581\n",
      "Epoch 29/30\n",
      "187/187 [==============================] - 85s 452ms/step - loss: 1.3070 - accuracy: 0.8621\n",
      "Epoch 30/30\n",
      "187/187 [==============================] - 75s 400ms/step - loss: 1.3016 - accuracy: 0.8675\n",
      "47/47 [==============================] - 5s 38ms/step - loss: 1.6837 - accuracy: 0.4773\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\textract_text()\n",
    "\ttextlist = read_text_file(\"data.txt\")\n",
    "\tlabel_list = extract_labels(textlist)\n",
    "\tmsg_list = extract_text_msgs(textlist)\n",
    "\tword_to_index,index_to_word,word_to_vec = read_glove_vector('glove.6B.50d.txt')\n",
    "\tx_train, x_test, y_train, y_test = train_test_split(msg_list, label_list,stratify = label_list,\\\n",
    "\t\ttest_size = 0.2, random_state = 123)\n",
    "\ttk = Tokenizer(lower = True, filters='')\n",
    "\ttk.fit_on_texts(msg_list)\n",
    "\ttrain_tokenized = tk.texts_to_sequences(x_train)\n",
    "\ttest_tokenized = tk.texts_to_sequences(x_test)\n",
    "\tmaxlen = 50\n",
    "\tX_train = pad_sequences(train_tokenized, maxlen = maxlen)\n",
    "\tX_test = pad_sequences(test_tokenized, maxlen = maxlen)\n",
    "\tif os.path.exists('tokenizer.pickle'):\n",
    "\t\tos.remove('tokenizer.pickle')\n",
    "\t\twith open('tokenizer.pickle', 'wb') as tokenizer:\n",
    "\t\t\tpickle.dump(tk, tokenizer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\tembedding_layer = create_embedding_layer(word_to_index,word_to_vec)\n",
    "\tmodel = create_lstm_model((maxlen,),embedding_layer)\n",
    "\tprint(model.summary())\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tmodel.fit(X_train, np.array(y_train), epochs = 30, batch_size = 32, shuffle=True)\n",
    "\tmodel.save('emoji_model.h5')\n",
    "\tmodel = load_model('emoji_model.h5')\n",
    "\tloss, acc = model.evaluate(X_test, np.array(y_test))\n",
    "\ttest_sent = tk.texts_to_sequences(['Feeling sad that my favourite cricketer has retired'])\n",
    "\ttest_sent = pad_sequences(test_sent, maxlen = maxlen)\n",
    "\tpred = model.predict(test_sent)\n",
    "\tprint(np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b889d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09408e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
